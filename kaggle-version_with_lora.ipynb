{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consumer Complaint Classification Pipeline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraModel ,LoraConfig, TaskType, PrefixEncoder, PrefixTuningConfig, IA3Config, IA3Model\n",
    "from torch.utils.data import Dataset\n",
    "import wandb\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "# =====================================\n",
    "# W&B AUTHENTICATION\n",
    "# =====================================\n",
    "\n",
    "# Insert your W&B API key here\n",
    "WANDB_API_KEY = \"9a1fd6047bfcfd9078f5984adb7e658960eb9509\"  # Replace with your actual API key\n",
    "\n",
    "# Login to W&B\n",
    "wandb.login(key=WANDB_API_KEY)\n",
    "\n",
    "# =====================================\n",
    "# CONFIGURATION & SETUP\n",
    "# =====================================\n",
    "\n",
    "class Config:\n",
    "    # Model Configuration - Choose appropriate pre-trained model\n",
    "    MODEL_NAME = \"roberta-base\"  # Efficient for English text classification\n",
    "    MAX_LENGTH = 512\n",
    "    \n",
    "    # Training Configuration (Fixed hyperparameters - no tuning)\n",
    "    LEARNING_RATE = 2e-5  # Lower learning rate as suggested by professor\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 5  # More epochs to observe learning curves and plateaus\n",
    "    WARMUP_STEPS = 500\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    \n",
    "    # Statistical Robustness Configuration\n",
    "    NUM_ITERATIONS = 5  # Different train/val/test splits\n",
    "    SAMPLE_SIZE_PER_CLASS = 2500  # Balanced sampling per class\n",
    "    \n",
    "    # Monitoring & Logging\n",
    "    LOGGING_STEPS = 50  # Frequent logging for real-time metrics\n",
    "    EVAL_STEPS = 100    # Evaluate every 100 steps\n",
    "    SAVE_STEPS = 200\n",
    "    \n",
    "    # Paths\n",
    "    DATA_PATH = './data/complaints.csv'  # make sure the file exists here\n",
    "    OUTPUT_DIR = './outputs'  # local directory you can write to\n",
    "    \n",
    "    # W&B Configuration\n",
    "    WANDB_PROJECT = \"consumer-complaints-robustness\"\n",
    "    WANDB_ENTITY = None  # Set your W&B username if needed\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seed for reproducibility across all iterations\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "def setup_wandb(iteration):\n",
    "    \"\"\"Initialize W&B for tracking real-time metrics\"\"\"\n",
    "    wandb.init(\n",
    "        project=Config.WANDB_PROJECT,\n",
    "        entity=Config.WANDB_ENTITY,\n",
    "        name=f\"iteration_{iteration + 1}\",\n",
    "        config={\n",
    "            \"model\": Config.MODEL_NAME,\n",
    "            \"learning_rate\": Config.LEARNING_RATE,\n",
    "            \"batch_size\": Config.BATCH_SIZE,\n",
    "            \"epochs\": Config.NUM_EPOCHS,\n",
    "            \"max_length\": Config.MAX_LENGTH,\n",
    "            \"iteration\": iteration + 1,\n",
    "            \"sample_size_per_class\": Config.SAMPLE_SIZE_PER_CLASS\n",
    "        },\n",
    "        reinit=True,\n",
    "        tags=[\"statistical_robustness\", \"transformer\", \"classification\"]\n",
    "    )\n",
    "\n",
    "# =====================================\n",
    "# DATASET PREPARATION\n",
    "# =====================================\n",
    "\n",
    "class ComplaintDataset(Dataset):\n",
    "    \"\"\"Custom dataset for consumer complaints\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare the consumer complaints dataset\"\"\"\n",
    "    print(\"📊 Loading and preparing dataset...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(Config.DATA_PATH)\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 0'])\n",
    "    \n",
    "    print(f\"   Original dataset size: {len(df)}\")\n",
    "    print(f\"   Original class distribution:\")\n",
    "    print(df['product_5'].value_counts())\n",
    "    \n",
    "    # Balance dataset by sampling from each class\n",
    "    df_balanced = df.groupby(\"product_5\").sample(\n",
    "        n=Config.SAMPLE_SIZE_PER_CLASS, \n",
    "        random_state=42,\n",
    "        replace=False  # Ensure no duplicates\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(df_balanced['product_5'])\n",
    "    \n",
    "    # Create label mapping\n",
    "    label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "    \n",
    "    print(f\"✅ Balanced dataset prepared: {len(df_balanced)} samples\")\n",
    "    print(f\"📋 Classes ({len(label_mapping)}): {list(label_mapping.values())}\")\n",
    "    print(f\"📋 Samples per class: {Config.SAMPLE_SIZE_PER_CLASS}\")\n",
    "    \n",
    "    return df_balanced['narrative'].values, encoded_labels, label_mapping, label_encoder\n",
    "\n",
    "def create_stratified_splits(texts, labels, iteration):\n",
    "    \"\"\"Create stratified train/validation/test splits for statistical robustness\"\"\"\n",
    "    \n",
    "    # Use different random state for each iteration to ensure different splits\n",
    "    random_state = 42 + iteration * 10\n",
    "    \n",
    "    # First split: 70% train, 30% temp\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        texts, labels, \n",
    "        test_size=0.3, \n",
    "        random_state=random_state, \n",
    "        stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Second split: 15% validation, 15% test (from the 30% temp)\n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels,\n",
    "        test_size=0.5,  # 50% of 30% = 15% of total\n",
    "        random_state=random_state,\n",
    "        stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"🔄 Split {iteration + 1}: Train={len(train_texts)}, Val={len(val_texts)}, Test={len(test_texts)}\")\n",
    "    \n",
    "    # Verify class distribution is maintained\n",
    "    unique_train, counts_train = np.unique(train_labels, return_counts=True)\n",
    "    unique_val, counts_val = np.unique(val_labels, return_counts=True)\n",
    "    unique_test, counts_test = np.unique(test_labels, return_counts=True)\n",
    "    \n",
    "    print(f\"   Train classes: {dict(zip(unique_train, counts_train))}\")\n",
    "    print(f\"   Val classes: {dict(zip(unique_val, counts_val))}\")\n",
    "    print(f\"   Test classes: {dict(zip(unique_test, counts_test))}\")\n",
    "    \n",
    "    return (train_texts, train_labels), (val_texts, val_labels), (test_texts, test_labels)\n",
    "\n",
    "# =====================================\n",
    "# MODEL & TRAINING SETUP\n",
    "# =====================================\n",
    "\n",
    "def setup_model_and_tokenizer(num_labels, model_type=\"distilbert\", peft_type=\"none\"):\n",
    "    \"\"\"Initialize model, tokenizer, and wrap with PEFT if specified\"\"\"\n",
    "    print(f\"🤖 Loading model: {Config.MODEL_NAME}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        Config.MODEL_NAME,\n",
    "        num_labels=num_labels,\n",
    "        problem_type=\"single_label_classification\"\n",
    "    )\n",
    "    \n",
    "    # Add pad token if missing\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    # Determine target modules for PEFT\n",
    "    if peft_type.lower() == \"lora\":\n",
    "        if model_type == \"distilbert\":\n",
    "            target_modules = [\"q_lin\", \"v_lin\"]\n",
    "        elif model_type == \"roberta\":\n",
    "            target_modules = [\"q\", \"v\"]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model_type for LoRA: {model_type}\")\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=target_modules,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\"\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "\n",
    "    elif peft_type.lower() == \"prefix\":\n",
    "        prefix_config = PrefixTuningConfig(\n",
    "            peft_type=\"PREFIX_TUNING\",\n",
    "            task_type=\"SEQ_CLS\",\n",
    "            num_virtual_tokens=20,\n",
    "            token_dim=model.config.hidden_size,\n",
    "            num_attention_heads=model.config.num_attention_heads,\n",
    "            num_layers=model.config.num_hidden_layers,\n",
    "        )\n",
    "        model = get_peft_model(model, prefix_config)\n",
    "\n",
    "    elif peft_type.lower() == \"ia3\":\n",
    "        ia3_config = IA3Config(\n",
    "            peft_type=\"IA3\",\n",
    "            task_type=\"SEQ_CLS\",\n",
    "            target_modules=[\"k\", \"v\"],\n",
    "            feedforward_modules=[\"intermediate.dense\"]\n",
    "        )\n",
    "        model = get_peft_model(model, ia3_config)\n",
    "\n",
    "    else:\n",
    "        print(\"⚠️ No PEFT applied. Using full fine-tuning.\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute comprehensive metrics for evaluation\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted', zero_division=0\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Per-class metrics for detailed analysis\n",
    "    precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_per_class_mean': np.mean(f1_per_class),\n",
    "        'precision_per_class_mean': np.mean(precision_per_class),\n",
    "        'recall_per_class_mean': np.mean(recall_per_class)\n",
    "    }\n",
    "\n",
    "# =====================================\n",
    "# TRAINING PIPELINE\n",
    "# =====================================\n",
    "\n",
    "def train_single_iteration(iteration, train_data, val_data, test_data, \n",
    "                         model, tokenizer, label_mapping):\n",
    "    \"\"\"Train model for a single iteration and evaluate performance\"\"\"\n",
    "    \n",
    "    print(f\"\\n🚀 Starting training for iteration {iteration + 1}...\")\n",
    "    \n",
    "    # Initialize W&B for this iteration\n",
    "    setup_wandb(iteration)\n",
    "    \n",
    "    # Create datasets from data tuples\n",
    "    train_dataset = ComplaintDataset(*train_data, tokenizer, Config.MAX_LENGTH)\n",
    "    val_dataset = ComplaintDataset(*val_data, tokenizer, Config.MAX_LENGTH)\n",
    "    test_dataset = ComplaintDataset(*test_data, tokenizer, Config.MAX_LENGTH)\n",
    "    \n",
    "    # Define iteration-specific paths\n",
    "    iter_dir = f\"{Config.OUTPUT_DIR}/iteration_{iteration + 1}\"\n",
    "    \n",
    "    # Setup training arguments with aligned evaluation and saving strategies\n",
    "    training_args = TrainingArguments(\n",
    "        # Basic configuration\n",
    "        output_dir=iter_dir,\n",
    "        num_train_epochs=Config.NUM_EPOCHS,\n",
    "        per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=Config.BATCH_SIZE,\n",
    "        \n",
    "        # Optimization parameters\n",
    "        learning_rate=Config.LEARNING_RATE,\n",
    "        weight_decay=Config.WEIGHT_DECAY,\n",
    "        warmup_steps=Config.WARMUP_STEPS,\n",
    "        \n",
    "        # Evaluation and saving - MUST use the same strategy when using load_best_model_at_end\n",
    "        eval_strategy=\"steps\",  # Changed from \"no\" to \"steps\"\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=Config.EVAL_STEPS,\n",
    "        save_steps=Config.SAVE_STEPS,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=3,\n",
    "        \n",
    "        # Logging\n",
    "        logging_dir=f\"{iter_dir}/logs\",\n",
    "        logging_steps=Config.LOGGING_STEPS,\n",
    "        logging_first_step=True,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=f\"iteration_{iteration + 1}\",\n",
    "        \n",
    "        # System settings\n",
    "        dataloader_num_workers=0,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        seed=42 + iteration,\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer with early stopping\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)]\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate\n",
    "    print(f\"   Training with {len(train_dataset)} samples...\")\n",
    "    train_result = trainer.train()\n",
    "    test_results = trainer.evaluate(test_dataset, metric_key_prefix=\"test\")\n",
    "    \n",
    "    # Log training metrics to W&B\n",
    "    wandb.log({\n",
    "        \"train_runtime\": train_result.metrics.get(\"train_runtime\", 0),\n",
    "        \"train_samples_per_second\": train_result.metrics.get(\"train_samples_per_second\", 0),\n",
    "        \"total_train_steps\": train_result.global_step,\n",
    "        \"iteration\": iteration + 1\n",
    "    })\n",
    "    \n",
    "    # Generate predictions and classification report\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    class_names = [label_mapping[i] for i in range(len(label_mapping))]\n",
    "    report = classification_report(\n",
    "        test_data[1], y_pred, target_names=class_names, output_dict=True, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Log test results\n",
    "    wandb.log({\n",
    "        \"test_accuracy\": test_results[\"test_accuracy\"],\n",
    "        \"test_f1\": test_results[\"test_f1\"],\n",
    "        \"test_precision\": test_results[\"test_precision\"],\n",
    "        \"test_recall\": test_results[\"test_recall\"]\n",
    "    })\n",
    "    \n",
    "    # Log per-class metrics\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        if str(i) in report:\n",
    "            class_key = class_name.lower().replace(' ', '_')\n",
    "            wandb.log({\n",
    "                f\"test_{class_key}_f1\": report[str(i)]['f1-score'],\n",
    "                f\"test_{class_key}_precision\": report[str(i)]['precision'],\n",
    "                f\"test_{class_key}_recall\": report[str(i)]['recall']\n",
    "            })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"📈 Results: Acc={test_results['test_accuracy']:.4f}, F1={test_results['test_f1']:.4f}\")\n",
    "    \n",
    "    # Save model and tokenizer\n",
    "    model_path = f\"{Config.OUTPUT_DIR}/best_model_iteration_{iteration + 1}\"\n",
    "    trainer.save_model(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    # Return key metrics\n",
    "    return {\n",
    "        'accuracy': test_results[\"test_accuracy\"],\n",
    "        'f1': test_results[\"test_f1\"],\n",
    "        'precision': test_results[\"test_precision\"],\n",
    "        'recall': test_results[\"test_recall\"],\n",
    "        'classification_report': report,\n",
    "        'train_steps': train_result.global_step,\n",
    "        'model_path': model_path\n",
    "    }\n",
    "\n",
    "# =====================================\n",
    "# MAIN PIPELINE EXECUTION\n",
    "# =====================================\n",
    "\n",
    "def create_results_summary(all_results):\n",
    "    \"\"\"Create comprehensive results summary\"\"\"\n",
    "    \n",
    "    # Extract metrics\n",
    "    accuracies = [r['accuracy'] for r in all_results]\n",
    "    f1_scores = [r['f1'] for r in all_results]\n",
    "    precisions = [r['precision'] for r in all_results]\n",
    "    recalls = [r['recall'] for r in all_results]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    results_summary = {\n",
    "        'accuracy': {\n",
    "            'mean': np.mean(accuracies),\n",
    "            'std': np.std(accuracies),\n",
    "            'min': np.min(accuracies),\n",
    "            'max': np.max(accuracies),\n",
    "            'values': accuracies\n",
    "        },\n",
    "        'f1': {\n",
    "            'mean': np.mean(f1_scores),\n",
    "            'std': np.std(f1_scores),\n",
    "            'min': np.min(f1_scores),\n",
    "            'max': np.max(f1_scores),\n",
    "            'values': f1_scores\n",
    "        },\n",
    "        'precision': {\n",
    "            'mean': np.mean(precisions),\n",
    "            'std': np.std(precisions),\n",
    "            'min': np.min(precisions),\n",
    "            'max': np.max(precisions),\n",
    "            'values': precisions\n",
    "        },\n",
    "        'recall': {\n",
    "            'mean': np.mean(recalls),\n",
    "            'std': np.std(recalls),\n",
    "            'min': np.min(recalls),\n",
    "            'max': np.max(recalls),\n",
    "            'values': recalls\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results_summary\n",
    "\n",
    "def plot_results_distribution(results_summary):\n",
    "    \"\"\"Create visualization of results distribution\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle('Statistical Robustness Results Distribution', fontsize=16)\n",
    "    \n",
    "    metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
    "    colors = ['blue', 'green', 'orange', 'red']\n",
    "    \n",
    "    for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "        ax = axes[i//2, i%2]\n",
    "        values = results_summary[metric]['values']\n",
    "        \n",
    "        # Box plot\n",
    "        ax.boxplot(values, patch_artist=True, \n",
    "                  boxprops=dict(facecolor=color, alpha=0.3))\n",
    "        ax.scatter(range(1, len(values)+1), values, \n",
    "                  color=color, alpha=0.7, s=50)\n",
    "        \n",
    "        # Add mean line\n",
    "        mean_val = results_summary[metric]['mean']\n",
    "        ax.axhline(y=mean_val, color=color, linestyle='--', alpha=0.8)\n",
    "        \n",
    "        ax.set_title(f'{metric.capitalize()} Distribution')\n",
    "        ax.set_ylabel(metric.capitalize())\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics text\n",
    "        std_val = results_summary[metric]['std']\n",
    "        ax.text(0.02, 0.98, f'Mean: {mean_val:.4f}\\nStd: {std_val:.4f}', \n",
    "               transform=ax.transAxes, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{Config.OUTPUT_DIR}/results_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Add new function to track and save only the best model\n",
    "def track_best_model(all_results, iteration, model, tokenizer):\n",
    "    \"\"\"Track and save only the best performing model\"\"\"\n",
    "    \n",
    "    current_accuracy = all_results[-1]['accuracy']\n",
    "    \n",
    "    # Check if this is the best model so far\n",
    "    if iteration == 0:  # First iteration\n",
    "        best_accuracy = current_accuracy\n",
    "        is_best = True\n",
    "    else:\n",
    "        best_accuracy = max([r['accuracy'] for r in all_results[:-1]])\n",
    "        is_best = current_accuracy > best_accuracy\n",
    "    \n",
    "    if is_best and Config.SAVE_ONLY_BEST_MODEL:\n",
    "        # Remove previous best model if exists\n",
    "        best_model_path = f\"{Config.OUTPUT_DIR}/best_model_overall\"\n",
    "        if os.path.exists(best_model_path):\n",
    "            shutil.rmtree(best_model_path)\n",
    "        \n",
    "        # Save current best model\n",
    "        os.makedirs(best_model_path, exist_ok=True)\n",
    "        model.save_pretrained(best_model_path)\n",
    "        tokenizer.save_pretrained(best_model_path)\n",
    "        \n",
    "        print(f\"💾 New best model saved! Accuracy: {current_accuracy:.4f}\")\n",
    "        \n",
    "        # Save best model info\n",
    "        best_info = {\n",
    "            'iteration': iteration + 1,\n",
    "            'accuracy': current_accuracy,\n",
    "            'model_path': best_model_path\n",
    "        }\n",
    "        \n",
    "        import json\n",
    "        with open(f\"{Config.OUTPUT_DIR}/best_model_info.json\", 'w') as f:\n",
    "            json.dump(best_info, f, indent=2)\n",
    "    \n",
    "    return is_best\n",
    "\n",
    "# =====================================\n",
    "# ERROR ANALYSIS\n",
    "# =====================================\n",
    "\n",
    "def perform_error_analysis(predictions, true_labels, texts, class_names, iteration):\n",
    "    \"\"\"\n",
    "    Perform comprehensive error analysis on model predictions\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions (class indices)\n",
    "        true_labels: Ground truth labels\n",
    "        texts: Original text samples\n",
    "        class_names: List of class names\n",
    "        iteration: Current iteration number\n",
    "    \n",
    "    Returns:\n",
    "        error_analysis_results: Dictionary with error analysis metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 ERROR ANALYSIS FOR ITERATION {iteration + 1}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    \n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    # Plot and save confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix - Iteration {iteration + 1}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the confusion matrix\n",
    "    os.makedirs(f\"{Config.OUTPUT_DIR}/error_analysis\", exist_ok=True)\n",
    "    plt.savefig(f\"{Config.OUTPUT_DIR}/error_analysis/confusion_matrix_iter_{iteration + 1}.png\", \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Find misclassified examples\n",
    "    misclassified_indices = np.where(predictions != true_labels)[0]\n",
    "    correct_indices = np.where(predictions == true_labels)[0]\n",
    "    \n",
    "    accuracy = len(correct_indices) / (len(correct_indices) + len(misclassified_indices))\n",
    "    \n",
    "    print(f\"   Total test samples: {len(true_labels)}\")\n",
    "    print(f\"   Correctly classified: {len(correct_indices)} ({accuracy:.2%})\")\n",
    "    print(f\"   Misclassified: {len(misclassified_indices)} ({1-accuracy:.2%})\")\n",
    "    \n",
    "    # Extract most common misclassifications\n",
    "    error_pairs = []\n",
    "    for idx in misclassified_indices:\n",
    "        true_class = class_names[true_labels[idx]]\n",
    "        pred_class = class_names[predictions[idx]]\n",
    "        error_pairs.append((true_class, pred_class))\n",
    "    \n",
    "    # Count frequency of each error type\n",
    "    error_counts = {}\n",
    "    for true_class, pred_class in error_pairs:\n",
    "        error_key = f\"{true_class} → {pred_class}\"\n",
    "        error_counts[error_key] = error_counts.get(error_key, 0) + 1\n",
    "    \n",
    "    # Sort by frequency\n",
    "    common_errors = sorted(error_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\n   Most common misclassifications:\")\n",
    "    for error_type, count in common_errors[:5]:  # Show top 5 errors\n",
    "        print(f\"   - {error_type}: {count} samples ({count/len(misclassified_indices):.1%} of errors)\")\n",
    "    \n",
    "    # Extract some example misclassifications\n",
    "    misclassified_samples = []\n",
    "    if len(misclassified_indices) > 0:\n",
    "        # Get a sample of misclassified examples (max 20)\n",
    "        sample_size = min(20, len(misclassified_indices))\n",
    "        sample_indices = np.random.choice(misclassified_indices, sample_size, replace=False)\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            misclassified_samples.append({\n",
    "                'text': texts[idx][:100] + \"...\" if len(texts[idx]) > 100 else texts[idx],\n",
    "                'true_label': class_names[true_labels[idx]],\n",
    "                'pred_label': class_names[predictions[idx]]\n",
    "            })\n",
    "    \n",
    "    # Class-wise error analysis\n",
    "    class_metrics = {}\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        # Find indices where true label is this class\n",
    "        class_indices = np.where(true_labels == i)[0]\n",
    "        if len(class_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Calculate accuracy for this class\n",
    "        class_correct = np.sum(predictions[class_indices] == true_labels[class_indices])\n",
    "        class_accuracy = class_correct / len(class_indices)\n",
    "        \n",
    "        # Find most common incorrect predictions for this class\n",
    "        class_errors = {}\n",
    "        for idx in class_indices:\n",
    "            if predictions[idx] != true_labels[idx]:\n",
    "                pred_class = class_names[predictions[idx]]\n",
    "                class_errors[pred_class] = class_errors.get(pred_class, 0) + 1\n",
    "        \n",
    "        # Sort errors by frequency\n",
    "        common_class_errors = sorted(class_errors.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        class_metrics[class_name] = {\n",
    "            'accuracy': class_accuracy,\n",
    "            'total_samples': len(class_indices),\n",
    "            'common_errors': common_class_errors[:3]  # Top 3 errors\n",
    "        }\n",
    "    \n",
    "    # Compile all error analysis results\n",
    "    error_analysis_results = {\n",
    "        'confusion_matrix': cm,\n",
    "        'accuracy': accuracy,\n",
    "        'error_rate': 1 - accuracy,\n",
    "        'common_errors': common_errors[:10],  # Top 10 most common errors\n",
    "        'misclassified_samples': misclassified_samples,\n",
    "        'class_metrics': class_metrics\n",
    "    }\n",
    "    \n",
    "    # Create error analysis report\n",
    "    report_path = f\"{Config.OUTPUT_DIR}/error_analysis/report_iter_{iteration + 1}.txt\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(f\"ERROR ANALYSIS REPORT - ITERATION {iteration + 1}\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Overall Accuracy: {accuracy:.4f}\\n\")\n",
    "        f.write(f\"Error Rate: {1-accuracy:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"MOST COMMON MISCLASSIFICATIONS:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        for error_type, count in common_errors[:10]:\n",
    "            f.write(f\"{error_type}: {count} samples ({count/len(misclassified_indices):.1%} of errors)\\n\")\n",
    "        \n",
    "        f.write(\"\\nCLASS-WISE PERFORMANCE:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        for class_name, metrics in class_metrics.items():\n",
    "            f.write(f\"\\n{class_name}:\\n\")\n",
    "            f.write(f\"  Accuracy: {metrics['accuracy']:.4f}\\n\")\n",
    "            f.write(f\"  Total Samples: {metrics['total_samples']}\\n\")\n",
    "            if metrics['common_errors']:\n",
    "                f.write(\"  Common errors:\\n\")\n",
    "                for error_class, count in metrics['common_errors']:\n",
    "                    f.write(f\"    - Confused as {error_class}: {count} times\\n\")\n",
    "    \n",
    "    print(f\"   Error analysis report saved to {report_path}\")\n",
    "    \n",
    "    return error_analysis_results\n",
    "\n",
    "# Function to aggregate error analysis across iterations\n",
    "def aggregate_error_analysis(all_error_analyses, label_mapping):\n",
    "    \"\"\"\n",
    "    Aggregate error analyses across multiple iterations\n",
    "    \"\"\"\n",
    "    print(\"\\n📊 AGGREGATING ERROR ANALYSIS ACROSS ITERATIONS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Aggregate common errors across iterations\n",
    "    all_common_errors = defaultdict(int)\n",
    "    for analysis in all_error_analyses:\n",
    "        for error_pair, count in analysis['common_errors']:\n",
    "            all_common_errors[error_pair] += count\n",
    "    \n",
    "    # Sort aggregated errors\n",
    "    aggregated_common_errors = sorted(all_common_errors.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Aggregate class-wise metrics\n",
    "    class_accuracies = defaultdict(list)\n",
    "    for analysis in all_error_analyses:\n",
    "        for class_name, metrics in analysis['class_metrics'].items():\n",
    "            class_accuracies[class_name].append(metrics['accuracy'])\n",
    "    \n",
    "    # Calculate mean and standard deviation of class accuracies\n",
    "    class_metrics_summary = {}\n",
    "    for class_name, accuracies in class_accuracies.items():\n",
    "        class_metrics_summary[class_name] = {\n",
    "            'mean_accuracy': np.mean(accuracies),\n",
    "            'std_accuracy': np.std(accuracies),\n",
    "            'min_accuracy': np.min(accuracies),\n",
    "            'max_accuracy': np.max(accuracies)\n",
    "        }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nMOST CHALLENGING CLASSIFICATION PAIRS (AGGREGATED):\")\n",
    "    for error_pair, total_count in aggregated_common_errors[:5]:\n",
    "        print(f\"   - {error_pair}: {total_count} total errors\")\n",
    "    \n",
    "    print(\"\\nCLASS-WISE ACCURACY SUMMARY:\")\n",
    "    for class_name, metrics in sorted(class_metrics_summary.items(), \n",
    "                                     key=lambda x: x[1]['mean_accuracy']):\n",
    "        print(f\"   - {class_name}: {metrics['mean_accuracy']:.4f} ± {metrics['std_accuracy']:.4f}\")\n",
    "    \n",
    "    # Create aggregate confusion matrix\n",
    "    class_names = [label_mapping[i] for i in range(len(label_mapping))]\n",
    "    aggregate_cm = np.zeros((len(class_names), len(class_names)))\n",
    "    for analysis in all_error_analyses:\n",
    "        aggregate_cm += analysis['confusion_matrix']\n",
    "    \n",
    "    # Plot aggregate confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(aggregate_cm, annot=True, fmt='.1f', cmap='Blues',\n",
    "               xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Aggregate Confusion Matrix Across All Iterations')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{Config.OUTPUT_DIR}/error_analysis/aggregate_confusion_matrix.png\", \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Generate aggregate report\n",
    "    report_path = f\"{Config.OUTPUT_DIR}/error_analysis/aggregate_error_analysis.txt\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"AGGREGATE ERROR ANALYSIS ACROSS ALL ITERATIONS\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"MOST COMMON MISCLASSIFICATIONS:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        for error_pair, count in aggregated_common_errors[:15]:\n",
    "            f.write(f\"{error_pair}: {count} total errors\\n\")\n",
    "        \n",
    "        f.write(\"\\nCLASS-WISE PERFORMANCE SUMMARY:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        for class_name, metrics in sorted(class_metrics_summary.items(), \n",
    "                                         key=lambda x: x[1]['mean_accuracy']):\n",
    "            f.write(f\"\\n{class_name}:\\n\")\n",
    "            f.write(f\"  Mean Accuracy: {metrics['mean_accuracy']:.4f}\\n\")\n",
    "            f.write(f\"  Std Deviation: {metrics['std_accuracy']:.4f}\\n\")\n",
    "            f.write(f\"  Range: [{metrics['min_accuracy']:.4f} - {metrics['max_accuracy']:.4f}]\\n\")\n",
    "    \n",
    "    print(f\"\\n✅ Aggregate error analysis saved to {report_path}\")\n",
    "    \n",
    "    return {\n",
    "        'aggregate_confusion_matrix': aggregate_cm,\n",
    "        'common_errors': aggregated_common_errors,\n",
    "        'class_metrics': class_metrics_summary\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main pipeline execution with comprehensive statistical robustness analysis\"\"\"\n",
    "    \n",
    "    print(\"🔥 Consumer Complaint Classification - Statistical Robustness Pipeline\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Model: {Config.MODEL_NAME}\")\n",
    "    print(f\"Learning Rate: {Config.LEARNING_RATE}\")\n",
    "    print(f\"Epochs: {Config.NUM_EPOCHS}\")\n",
    "    print(f\"Iterations: {Config.NUM_ITERATIONS}\")\n",
    "    print(f\"Space optimization: {'Enabled' if not Config.SAVE_MODELS else 'Disabled'}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Set up environment\n",
    "    set_seed(42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"💻 Using device: {device}\")\n",
    "    \n",
    "    # Create minimal output directory structure\n",
    "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # Create directory for error analysis\n",
    "    os.makedirs(f\"{Config.OUTPUT_DIR}/error_analysis\", exist_ok=True)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    texts, labels, label_mapping, label_encoder = load_and_prepare_data()\n",
    "    \n",
    "    # Store results across iterations\n",
    "    all_results = []\n",
    "    all_error_analyses = []\n",
    "    \n",
    "    print(f\"\\n🔄 Starting {Config.NUM_ITERATIONS} iterations for statistical robustness...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Run multiple iterations with different splits\n",
    "    for iteration in range(Config.NUM_ITERATIONS):\n",
    "        print(f\"\\n📋 ITERATION {iteration + 1}/{Config.NUM_ITERATIONS}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create new splits for this iteration\n",
    "        train_data, val_data, test_data = create_stratified_splits(texts, labels, iteration)\n",
    "        \n",
    "        # Initialize fresh model for each iteration\n",
    "        model, tokenizer = setup_model_and_tokenizer(len(label_mapping))\n",
    "        model.to(device)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        results = train_single_iteration(\n",
    "            iteration, train_data, val_data, test_data, \n",
    "            model, tokenizer, label_mapping\n",
    "        )\n",
    "        \n",
    "        all_results.append(results)\n",
    "        all_error_analyses.append(results['error_analysis'])\n",
    "        \n",
    "        # Track and potentially save best model\n",
    "        if Config.SAVE_ONLY_BEST_MODEL:\n",
    "            is_best = track_best_model(all_results, iteration, model, tokenizer)\n",
    "            if is_best:\n",
    "                print(f\"⭐ This is the new best model!\")\n",
    "        \n",
    "        print(f\"✅ Iteration {iteration + 1} completed!\")\n",
    "        \n",
    "        # Clear CUDA cache and delete model to free memory\n",
    "        del model, tokenizer\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # =====================================\n",
    "    # AGGREGATE RESULTS & ANALYSIS\n",
    "    # =====================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"📊 FINAL STATISTICAL ROBUSTNESS ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create comprehensive results summary\n",
    "    results_summary = create_results_summary(all_results)\n",
    "    \n",
    "    # Print aggregated statistics\n",
    "    print(f\"\\n📈 AGGREGATED METRICS ACROSS {Config.NUM_ITERATIONS} ITERATIONS:\")\n",
    "    print(\"-\" * 60)\n",
    "    for metric in ['accuracy', 'f1', 'precision', 'recall']:\n",
    "        stats = results_summary[metric]\n",
    "        print(f\"{metric.capitalize():12}: {stats['mean']:.4f} ± {stats['std']:.4f} \"\n",
    "              f\"[{stats['min']:.4f} - {stats['max']:.4f}]\")\n",
    "    \n",
    "    print(f\"\\n📊 INDIVIDUAL ITERATION RESULTS:\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, results in enumerate(all_results):\n",
    "        print(f\"Iteration {i+1:2d}: Acc={results['accuracy']:.4f}, \"\n",
    "              f\"F1={results['f1']:.4f}, Prec={results['precision']:.4f}, \"\n",
    "              f\"Rec={results['recall']:.4f}\")\n",
    "    \n",
    "    # Perform aggregate error analysis\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🔍 COMPREHENSIVE ERROR ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    aggregate_errors = aggregate_error_analysis(all_error_analyses, label_mapping)\n",
    "\n",
    "    # Create visualization\n",
    "    plot_results_distribution(results_summary)\n",
    "    \n",
    "    # Save only essential results (lightweight)\n",
    "    results_df = pd.DataFrame([\n",
    "        {\n",
    "            'iteration': r['iteration'],\n",
    "            'accuracy': r['accuracy'],\n",
    "            'f1': r['f1'],\n",
    "            'precision': r['precision'],\n",
    "            'recall': r['recall'],\n",
    "            'train_steps': r['train_steps']\n",
    "        }\n",
    "        for r in all_results\n",
    "    ])\n",
    "    \n",
    "    results_df.to_csv(f'{Config.OUTPUT_DIR}/detailed_results.csv', index=False)\n",
    "    \n",
    "    # Save summary statistics\n",
    "    summary_df = pd.DataFrame([\n",
    "        {\n",
    "            'metric': metric,\n",
    "            'mean': stats['mean'],\n",
    "            'std': stats['std'],\n",
    "            'min': stats['min'],\n",
    "            'max': stats['max']\n",
    "        }\n",
    "        for metric, stats in results_summary.items()\n",
    "    ])\n",
    "    \n",
    "    summary_df.to_csv(f'{Config.OUTPUT_DIR}/summary_statistics.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n✅ Pipeline completed successfully!\")\n",
    "    print(f\"📁 Results saved in: {Config.OUTPUT_DIR}\")\n",
    "    print(f\"📈 Check Weights & Biases project: {Config.WANDB_PROJECT}\")\n",
    "    print(f\"📊 Visualizations saved as: {Config.OUTPUT_DIR}/results_distribution.png\")\n",
    "    print(f\"🔍 Error analysis saved in: {Config.OUTPUT_DIR}/error_analysis/\")\n",
    "    \n",
    "    if Config.SAVE_ONLY_BEST_MODEL:\n",
    "        print(f\"💾 Best model saved in: {Config.OUTPUT_DIR}/best_model_overall\")\n",
    "    \n",
    "    # Final recommendations based on results\n",
    "    acc_std = results_summary['accuracy']['std']\n",
    "    if acc_std < 0.01:\n",
    "        print(\"\\n🎯 CONCLUSION: Model shows STRONG statistical robustness (low variance)\")\n",
    "    elif acc_std < 0.02:\n",
    "        print(\"\\n⚖️  CONCLUSION: Model shows MODERATE statistical robustness\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  CONCLUSION: Model shows HIGH variance - consider more data or different approach\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
