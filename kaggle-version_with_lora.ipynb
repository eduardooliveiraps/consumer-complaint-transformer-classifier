{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consumer Complaint Classification Pipeline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraModel ,LoraConfig, TaskType, PrefixEncoder, PrefixTuningConfig, IA3Config, IA3Model\n",
    "from torch.utils.data import Dataset\n",
    "import wandb\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# =====================================\n",
    "# W&B AUTHENTICATION\n",
    "# =====================================\n",
    "\n",
    "# Insert your W&B API key here\n",
    "WANDB_API_KEY = \"9a1fd6047bfcfd9078f5984adb7e658960eb9509\"  # Replace with your actual API key\n",
    "\n",
    "# Login to W&B\n",
    "wandb.login(key=WANDB_API_KEY)\n",
    "\n",
    "# =====================================\n",
    "# CONFIGURATION & SETUP\n",
    "# =====================================\n",
    "\n",
    "class Config:\n",
    "    # Model Configuration - Choose appropriate pre-trained model\n",
    "    MODEL_NAME = \"roberta-base\"  # Efficient for English text classification\n",
    "    MAX_LENGTH = 512\n",
    "    \n",
    "    # Training Configuration (Fixed hyperparameters - no tuning)\n",
    "    LEARNING_RATE = 2e-5  # Lower learning rate as suggested by professor\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 5  # More epochs to observe learning curves and plateaus\n",
    "    WARMUP_STEPS = 500\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    \n",
    "    # Statistical Robustness Configuration\n",
    "    NUM_ITERATIONS = 5  # Different train/val/test splits\n",
    "    SAMPLE_SIZE_PER_CLASS = 2500  # Balanced sampling per class\n",
    "    \n",
    "    # Monitoring & Logging\n",
    "    LOGGING_STEPS = 50  # Frequent logging for real-time metrics\n",
    "    EVAL_STEPS = 100    # Evaluate every 100 steps\n",
    "    SAVE_STEPS = 200\n",
    "    \n",
    "    # Paths\n",
    "    DATA_PATH = './data/complaints.csv'  # make sure the file exists here\n",
    "    OUTPUT_DIR = './outputs'  # local directory you can write to\n",
    "    \n",
    "    # W&B Configuration\n",
    "    WANDB_PROJECT = \"consumer-complaints-robustness\"\n",
    "    WANDB_ENTITY = None  # Set your W&B username if needed\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seed for reproducibility across all iterations\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "def setup_wandb(iteration):\n",
    "    \"\"\"Initialize W&B for tracking real-time metrics\"\"\"\n",
    "    wandb.init(\n",
    "        project=Config.WANDB_PROJECT,\n",
    "        entity=Config.WANDB_ENTITY,\n",
    "        name=f\"iteration_{iteration + 1}\",\n",
    "        config={\n",
    "            \"model\": Config.MODEL_NAME,\n",
    "            \"learning_rate\": Config.LEARNING_RATE,\n",
    "            \"batch_size\": Config.BATCH_SIZE,\n",
    "            \"epochs\": Config.NUM_EPOCHS,\n",
    "            \"max_length\": Config.MAX_LENGTH,\n",
    "            \"iteration\": iteration + 1,\n",
    "            \"sample_size_per_class\": Config.SAMPLE_SIZE_PER_CLASS\n",
    "        },\n",
    "        reinit=True,\n",
    "        tags=[\"statistical_robustness\", \"transformer\", \"classification\"]\n",
    "    )\n",
    "\n",
    "# =====================================\n",
    "# DATASET PREPARATION\n",
    "# =====================================\n",
    "\n",
    "class ComplaintDataset(Dataset):\n",
    "    \"\"\"Custom dataset for consumer complaints\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare the consumer complaints dataset\"\"\"\n",
    "    print(\"📊 Loading and preparing dataset...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(Config.DATA_PATH)\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 0'])\n",
    "    \n",
    "    print(f\"   Original dataset size: {len(df)}\")\n",
    "    print(f\"   Original class distribution:\")\n",
    "    print(df['product_5'].value_counts())\n",
    "    \n",
    "    # Balance dataset by sampling from each class\n",
    "    df_balanced = df.groupby(\"product_5\").sample(\n",
    "        n=Config.SAMPLE_SIZE_PER_CLASS, \n",
    "        random_state=42,\n",
    "        replace=False  # Ensure no duplicates\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(df_balanced['product_5'])\n",
    "    \n",
    "    # Create label mapping\n",
    "    label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "    \n",
    "    print(f\"✅ Balanced dataset prepared: {len(df_balanced)} samples\")\n",
    "    print(f\"📋 Classes ({len(label_mapping)}): {list(label_mapping.values())}\")\n",
    "    print(f\"📋 Samples per class: {Config.SAMPLE_SIZE_PER_CLASS}\")\n",
    "    \n",
    "    return df_balanced['narrative'].values, encoded_labels, label_mapping, label_encoder\n",
    "\n",
    "def create_stratified_splits(texts, labels, iteration):\n",
    "    \"\"\"Create stratified train/validation/test splits for statistical robustness\"\"\"\n",
    "    \n",
    "    # Use different random state for each iteration to ensure different splits\n",
    "    random_state = 42 + iteration * 10\n",
    "    \n",
    "    # First split: 70% train, 30% temp\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        texts, labels, \n",
    "        test_size=0.3, \n",
    "        random_state=random_state, \n",
    "        stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Second split: 15% validation, 15% test (from the 30% temp)\n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels,\n",
    "        test_size=0.5,  # 50% of 30% = 15% of total\n",
    "        random_state=random_state,\n",
    "        stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"🔄 Split {iteration + 1}: Train={len(train_texts)}, Val={len(val_texts)}, Test={len(test_texts)}\")\n",
    "    \n",
    "    # Verify class distribution is maintained\n",
    "    unique_train, counts_train = np.unique(train_labels, return_counts=True)\n",
    "    unique_val, counts_val = np.unique(val_labels, return_counts=True)\n",
    "    unique_test, counts_test = np.unique(test_labels, return_counts=True)\n",
    "    \n",
    "    print(f\"   Train classes: {dict(zip(unique_train, counts_train))}\")\n",
    "    print(f\"   Val classes: {dict(zip(unique_val, counts_val))}\")\n",
    "    print(f\"   Test classes: {dict(zip(unique_test, counts_test))}\")\n",
    "    \n",
    "    return (train_texts, train_labels), (val_texts, val_labels), (test_texts, test_labels)\n",
    "\n",
    "# =====================================\n",
    "# MODEL & TRAINING SETUP\n",
    "# =====================================\n",
    "\n",
    "def setup_model_and_tokenizer(num_labels, model_type=\"distilbert\", peft_type=\"none\"):\n",
    "    \"\"\"Initialize model, tokenizer, and wrap with PEFT if specified\"\"\"\n",
    "    print(f\"🤖 Loading model: {Config.MODEL_NAME}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        Config.MODEL_NAME,\n",
    "        num_labels=num_labels,\n",
    "        problem_type=\"single_label_classification\"\n",
    "    )\n",
    "    \n",
    "    # Add pad token if missing\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    # Determine target modules for PEFT\n",
    "    if peft_type.lower() == \"lora\":\n",
    "        if model_type == \"distilbert\":\n",
    "            target_modules = [\"q_lin\", \"v_lin\"]\n",
    "        elif model_type == \"roberta\":\n",
    "            target_modules = [\"q\", \"v\"]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model_type for LoRA: {model_type}\")\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=target_modules,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\"\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "\n",
    "    elif peft_type.lower() == \"prefix\":\n",
    "        prefix_config = PrefixTuningConfig(\n",
    "            peft_type=\"PREFIX_TUNING\",\n",
    "            task_type=\"SEQ_CLS\",\n",
    "            num_virtual_tokens=20,\n",
    "            token_dim=model.config.hidden_size,\n",
    "            num_attention_heads=model.config.num_attention_heads,\n",
    "            num_layers=model.config.num_hidden_layers,\n",
    "        )\n",
    "        model = get_peft_model(model, prefix_config)\n",
    "\n",
    "    elif peft_type.lower() == \"ia3\":\n",
    "        ia3_config = IA3Config(\n",
    "            peft_type=\"IA3\",\n",
    "            task_type=\"SEQ_CLS\",\n",
    "            target_modules=[\"k\", \"v\"],\n",
    "            feedforward_modules=[\"intermediate.dense\"]\n",
    "        )\n",
    "        model = get_peft_model(model, ia3_config)\n",
    "\n",
    "    else:\n",
    "        print(\"⚠️ No PEFT applied. Using full fine-tuning.\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute comprehensive metrics for evaluation\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted', zero_division=0\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Per-class metrics for detailed analysis\n",
    "    precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_per_class_mean': np.mean(f1_per_class),\n",
    "        'precision_per_class_mean': np.mean(precision_per_class),\n",
    "        'recall_per_class_mean': np.mean(recall_per_class)\n",
    "    }\n",
    "\n",
    "# =====================================\n",
    "# TRAINING PIPELINE\n",
    "# =====================================\n",
    "\n",
    "def train_single_iteration(iteration, train_data, val_data, test_data, \n",
    "                         model, tokenizer, label_mapping):\n",
    "    \"\"\"Train model for a single iteration and evaluate performance\"\"\"\n",
    "    \n",
    "    print(f\"\\n🚀 Starting training for iteration {iteration + 1}...\")\n",
    "    \n",
    "    # Initialize W&B for this iteration\n",
    "    setup_wandb(iteration)\n",
    "    \n",
    "    # Create datasets from data tuples\n",
    "    train_dataset = ComplaintDataset(*train_data, tokenizer, Config.MAX_LENGTH)\n",
    "    val_dataset = ComplaintDataset(*val_data, tokenizer, Config.MAX_LENGTH)\n",
    "    test_dataset = ComplaintDataset(*test_data, tokenizer, Config.MAX_LENGTH)\n",
    "    \n",
    "    # Define iteration-specific paths\n",
    "    iter_dir = f\"{Config.OUTPUT_DIR}/iteration_{iteration + 1}\"\n",
    "    \n",
    "    # Setup training arguments with aligned evaluation and saving strategies\n",
    "    training_args = TrainingArguments(\n",
    "        # Basic configuration\n",
    "        output_dir=iter_dir,\n",
    "        num_train_epochs=Config.NUM_EPOCHS,\n",
    "        per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=Config.BATCH_SIZE,\n",
    "        \n",
    "        # Optimization parameters\n",
    "        learning_rate=Config.LEARNING_RATE,\n",
    "        weight_decay=Config.WEIGHT_DECAY,\n",
    "        warmup_steps=Config.WARMUP_STEPS,\n",
    "        \n",
    "        # Evaluation and saving - MUST use the same strategy when using load_best_model_at_end\n",
    "        eval_strategy=\"steps\",  # Changed from \"no\" to \"steps\"\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=Config.EVAL_STEPS,\n",
    "        save_steps=Config.SAVE_STEPS,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=3,\n",
    "        \n",
    "        # Logging\n",
    "        logging_dir=f\"{iter_dir}/logs\",\n",
    "        logging_steps=Config.LOGGING_STEPS,\n",
    "        logging_first_step=True,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=f\"iteration_{iteration + 1}\",\n",
    "        \n",
    "        # System settings\n",
    "        dataloader_num_workers=0,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        seed=42 + iteration,\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer with early stopping\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)]\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate\n",
    "    print(f\"   Training with {len(train_dataset)} samples...\")\n",
    "    train_result = trainer.train()\n",
    "    test_results = trainer.evaluate(test_dataset, metric_key_prefix=\"test\")\n",
    "    \n",
    "    # Log training metrics to W&B\n",
    "    wandb.log({\n",
    "        \"train_runtime\": train_result.metrics.get(\"train_runtime\", 0),\n",
    "        \"train_samples_per_second\": train_result.metrics.get(\"train_samples_per_second\", 0),\n",
    "        \"total_train_steps\": train_result.global_step,\n",
    "        \"iteration\": iteration + 1\n",
    "    })\n",
    "    \n",
    "    # Generate predictions and classification report\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    class_names = [label_mapping[i] for i in range(len(label_mapping))]\n",
    "    report = classification_report(\n",
    "        test_data[1], y_pred, target_names=class_names, output_dict=True, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Log test results\n",
    "    wandb.log({\n",
    "        \"test_accuracy\": test_results[\"test_accuracy\"],\n",
    "        \"test_f1\": test_results[\"test_f1\"],\n",
    "        \"test_precision\": test_results[\"test_precision\"],\n",
    "        \"test_recall\": test_results[\"test_recall\"]\n",
    "    })\n",
    "    \n",
    "    # Log per-class metrics\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        if str(i) in report:\n",
    "            class_key = class_name.lower().replace(' ', '_')\n",
    "            wandb.log({\n",
    "                f\"test_{class_key}_f1\": report[str(i)]['f1-score'],\n",
    "                f\"test_{class_key}_precision\": report[str(i)]['precision'],\n",
    "                f\"test_{class_key}_recall\": report[str(i)]['recall']\n",
    "            })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"📈 Results: Acc={test_results['test_accuracy']:.4f}, F1={test_results['test_f1']:.4f}\")\n",
    "    \n",
    "    # Save model and tokenizer\n",
    "    model_path = f\"{Config.OUTPUT_DIR}/best_model_iteration_{iteration + 1}\"\n",
    "    trainer.save_model(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    # Return key metrics\n",
    "    return {\n",
    "        'accuracy': test_results[\"test_accuracy\"],\n",
    "        'f1': test_results[\"test_f1\"],\n",
    "        'precision': test_results[\"test_precision\"],\n",
    "        'recall': test_results[\"test_recall\"],\n",
    "        'classification_report': report,\n",
    "        'train_steps': train_result.global_step,\n",
    "        'model_path': model_path\n",
    "    }\n",
    "\n",
    "# =====================================\n",
    "# MAIN PIPELINE EXECUTION\n",
    "# =====================================\n",
    "\n",
    "def create_results_summary(all_results):\n",
    "    \"\"\"Create comprehensive results summary\"\"\"\n",
    "    \n",
    "    # Extract metrics\n",
    "    accuracies = [r['accuracy'] for r in all_results]\n",
    "    f1_scores = [r['f1'] for r in all_results]\n",
    "    precisions = [r['precision'] for r in all_results]\n",
    "    recalls = [r['recall'] for r in all_results]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    results_summary = {\n",
    "        'accuracy': {\n",
    "            'mean': np.mean(accuracies),\n",
    "            'std': np.std(accuracies),\n",
    "            'min': np.min(accuracies),\n",
    "            'max': np.max(accuracies),\n",
    "            'values': accuracies\n",
    "        },\n",
    "        'f1': {\n",
    "            'mean': np.mean(f1_scores),\n",
    "            'std': np.std(f1_scores),\n",
    "            'min': np.min(f1_scores),\n",
    "            'max': np.max(f1_scores),\n",
    "            'values': f1_scores\n",
    "        },\n",
    "        'precision': {\n",
    "            'mean': np.mean(precisions),\n",
    "            'std': np.std(precisions),\n",
    "            'min': np.min(precisions),\n",
    "            'max': np.max(precisions),\n",
    "            'values': precisions\n",
    "        },\n",
    "        'recall': {\n",
    "            'mean': np.mean(recalls),\n",
    "            'std': np.std(recalls),\n",
    "            'min': np.min(recalls),\n",
    "            'max': np.max(recalls),\n",
    "            'values': recalls\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results_summary\n",
    "\n",
    "def plot_results_distribution(results_summary):\n",
    "    \"\"\"Create visualization of results distribution\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle('Statistical Robustness Results Distribution', fontsize=16)\n",
    "    \n",
    "    metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
    "    colors = ['blue', 'green', 'orange', 'red']\n",
    "    \n",
    "    for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "        ax = axes[i//2, i%2]\n",
    "        values = results_summary[metric]['values']\n",
    "        \n",
    "        # Box plot\n",
    "        ax.boxplot(values, patch_artist=True, \n",
    "                  boxprops=dict(facecolor=color, alpha=0.3))\n",
    "        ax.scatter(range(1, len(values)+1), values, \n",
    "                  color=color, alpha=0.7, s=50)\n",
    "        \n",
    "        # Add mean line\n",
    "        mean_val = results_summary[metric]['mean']\n",
    "        ax.axhline(y=mean_val, color=color, linestyle='--', alpha=0.8)\n",
    "        \n",
    "        ax.set_title(f'{metric.capitalize()} Distribution')\n",
    "        ax.set_ylabel(metric.capitalize())\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics text\n",
    "        std_val = results_summary[metric]['std']\n",
    "        ax.text(0.02, 0.98, f'Mean: {mean_val:.4f}\\nStd: {std_val:.4f}', \n",
    "               transform=ax.transAxes, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{Config.OUTPUT_DIR}/results_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main pipeline execution with comprehensive statistical robustness analysis\"\"\"\n",
    "    \n",
    "    print(\"🔥 Consumer Complaint Classification - Statistical Robustness Pipeline\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Model: {Config.MODEL_NAME}\")\n",
    "    print(f\"Learning Rate: {Config.LEARNING_RATE}\")\n",
    "    print(f\"Epochs: {Config.NUM_EPOCHS}\")\n",
    "    print(f\"Iterations: {Config.NUM_ITERATIONS}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Set up environment\n",
    "    set_seed(42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"💻 Using device: {device}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    texts, labels, label_mapping, label_encoder = load_and_prepare_data()\n",
    "    \n",
    "    # Store results across iterations\n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"\\n🔄 Starting {Config.NUM_ITERATIONS} iterations for statistical robustness...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Run multiple iterations with different splits\n",
    "    for iteration in range(Config.NUM_ITERATIONS):\n",
    "        print(f\"\\n📋 ITERATION {iteration + 1}/{Config.NUM_ITERATIONS}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create new splits for this iteration\n",
    "        train_data, val_data, test_data = create_stratified_splits(texts, labels, iteration)\n",
    "        \n",
    "        # Initialize fresh model for each iteration\n",
    "        model, tokenizer = setup_model_and_tokenizer(len(label_mapping), \"roberta\")\n",
    "        model.to(device)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        results = train_single_iteration(\n",
    "            iteration, train_data, val_data, test_data, \n",
    "            model, tokenizer, label_mapping\n",
    "        )\n",
    "        \n",
    "        all_results.append(results)\n",
    "        print(f\"✅ Iteration {iteration + 1} completed!\")\n",
    "        \n",
    "        # Clear CUDA cache to prevent memory issues\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # =====================================\n",
    "    # AGGREGATE RESULTS & ANALYSIS\n",
    "    # =====================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"📊 FINAL STATISTICAL ROBUSTNESS ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create comprehensive results summary\n",
    "    results_summary = create_results_summary(all_results)\n",
    "    \n",
    "    # Print aggregated statistics\n",
    "    print(f\"\\n📈 AGGREGATED METRICS ACROSS {Config.NUM_ITERATIONS} ITERATIONS:\")\n",
    "    print(\"-\" * 60)\n",
    "    for metric in ['accuracy', 'f1', 'precision', 'recall']:\n",
    "        stats = results_summary[metric]\n",
    "        print(f\"{metric.capitalize():12}: {stats['mean']:.4f} ± {stats['std']:.4f} \"\n",
    "              f\"[{stats['min']:.4f} - {stats['max']:.4f}]\")\n",
    "    \n",
    "    print(f\"\\n📊 INDIVIDUAL ITERATION RESULTS:\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, results in enumerate(all_results):\n",
    "        print(f\"Iteration {i+1:2d}: Acc={results['accuracy']:.4f}, \"\n",
    "              f\"F1={results['f1']:.4f}, Prec={results['precision']:.4f}, \"\n",
    "              f\"Rec={results['recall']:.4f}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    plot_results_distribution(results_summary)\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_df = pd.DataFrame([\n",
    "        {\n",
    "            'iteration': i+1,\n",
    "            'accuracy': r['accuracy'],\n",
    "            'f1': r['f1'],\n",
    "            'precision': r['precision'],\n",
    "            'recall': r['recall'],\n",
    "            'train_steps': r['train_steps'],\n",
    "            'model_path': r['model_path']\n",
    "        }\n",
    "        for i, r in enumerate(all_results)\n",
    "    ])\n",
    "    \n",
    "    results_df.to_csv(f'{Config.OUTPUT_DIR}/detailed_results.csv', index=False)\n",
    "    \n",
    "    # Save summary statistics\n",
    "    summary_df = pd.DataFrame([\n",
    "        {\n",
    "            'metric': metric,\n",
    "            'mean': stats['mean'],\n",
    "            'std': stats['std'],\n",
    "            'min': stats['min'],\n",
    "            'max': stats['max']\n",
    "        }\n",
    "        for metric, stats in results_summary.items()\n",
    "    ])\n",
    "    \n",
    "    summary_df.to_csv(f'{Config.OUTPUT_DIR}/summary_statistics.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n✅ Pipeline completed successfully!\")\n",
    "    print(f\"📁 Results saved in: {Config.OUTPUT_DIR}\")\n",
    "    print(f\"📈 Check Weights & Biases project: {Config.WANDB_PROJECT}\")\n",
    "    print(f\"📊 Visualizations saved as: {Config.OUTPUT_DIR}/results_distribution.png\")\n",
    "    \n",
    "    # Final recommendations based on results\n",
    "    acc_std = results_summary['accuracy']['std']\n",
    "    if acc_std < 0.01:\n",
    "        print(\"\\n🎯 CONCLUSION: Model shows STRONG statistical robustness (low variance)\")\n",
    "    elif acc_std < 0.02:\n",
    "        print(\"\\n⚖️  CONCLUSION: Model shows MODERATE statistical robustness\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  CONCLUSION: Model shows HIGH variance - consider more data or different approach\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
