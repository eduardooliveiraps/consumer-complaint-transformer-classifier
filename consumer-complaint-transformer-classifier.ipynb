{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2192f595",
   "metadata": {},
   "source": [
    "# Consumer Complaint Classification â€“ Transformers \n",
    "\n",
    "## Introduction  \n",
    "\n",
    "The goal of this project is to develop a **text classification model** that categorizes **consumer complaints** into five financial categories using **transformer-based algorithms**.  \n",
    "\n",
    "The dataset, obtained from the **Consumer Financial Protection Bureau (CFPB)**, contains **over 2 million consumer complaints** from **2011 to 2024**. Each complaint is a textual **narrative** describing a financial issue, and these complaints have been labeled into **five main categories**:  \n",
    "\n",
    "- **Loans**  \n",
    "- **Credit Reporting**  \n",
    "- **Bank Accounts & Services**  \n",
    "- **Debt Collection**  \n",
    "- **Credit Card Services**  \n",
    "\n",
    "---\n",
    "\n",
    "## Data Understanding  \n",
    "\n",
    "The dataset originates from the **Consumer Complaint Database** maintained by the **Consumer Financial Protection Bureau (CFPB)**, a **U.S. federal agency** that mediates disputes between financial institutions and consumers. Consumers submit complaints through an **online form**, detailing their financial issues.  \n",
    "\n",
    "The dataset was **downloaded from the CFPB website** and underwent **preprocessing** to prepare it for NLP tasks. The key modifications include:  \n",
    "\n",
    "- Retaining only records where a **\"Consumer complaint narrative\"** is available.  \n",
    "- Reducing the dataset from **5,842,373** records to **2,023,066** entries.  \n",
    "- Renaming the **\"Consumer complaint narrative\"** column to **\"narrative\"** for ease of coding.  \n",
    "- Consolidating **18 original product categories** into **5 main categories (product_5)** to address overlaps in classification.  \n",
    "\n",
    "---\n",
    "\n",
    "## Consumer Complaint Classification Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fbd1110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/andre/.local/lib/python3.10/site-packages (4.46.2)\n",
      "Requirement already satisfied: datasets in /home/andre/.local/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: pandas in /home/andre/.local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/andre/.local/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /home/andre/.local/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: torch in /home/andre/.local/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: tqdm in /home/andre/.local/lib/python3.10/site-packages (4.66.5)\n",
      "Requirement already satisfied: filelock in /home/andre/.local/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/andre/.local/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/andre/.local/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/andre/.local/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/andre/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/andre/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/andre/.local/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/andre/.local/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/andre/.local/lib/python3.10/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/andre/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/andre/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/andre/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/andre/.local/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/andre/.local/lib/python3.10/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/andre/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/andre/.local/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/andre/.local/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/andre/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/andre/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions in /home/andre/.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/andre/.local/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/andre/.local/lib/python3.10/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /home/andre/.local/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/andre/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/andre/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/andre/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/andre/.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/andre/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/andre/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/andre/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/andre/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/andre/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/andre/.local/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/andre/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/andre/.local/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/andre/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/andre/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/andre/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/andre/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/andre/.local/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/andre/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/andre/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/andre/.local/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/andre/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/andre/.local/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andre/.local/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/andre/.local/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/andre/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 23:35:58.095476: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-19 23:35:58.240560: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747694158.298410   16624 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747694158.313827   16624 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-19 23:35:58.459896: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# !pip install transformers datasets pandas numpy scikit-learn torch tqdm\n",
    "\n",
    "# --------------------\n",
    "# Standard Libraries\n",
    "# --------------------\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "# --------------------\n",
    "# Data Manipulation\n",
    "# --------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --------------------\n",
    "# Visualization\n",
    "# --------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Machine Learning\n",
    "# --------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# --------------------\n",
    "# PyTorch & Transformers\n",
    "# --------------------\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    RobertaTokenizer, \n",
    "    RobertaForSequenceClassification\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# Progress Bar\n",
    "# --------------------\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --------------------\n",
    "# Set Seed for Reproducibility\n",
    "# --------------------\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# --------------------\n",
    "# Device Configuration\n",
    "# --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('data/complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88047f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'Unnamed: 0' column\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd16359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample size per category\n",
    "sample_size = 10000\n",
    "\n",
    "df_resampled = df.groupby(\"product_5\").sample(n=sample_size, random_state=42)\n",
    "\n",
    "df_resampled = df_resampled.reset_index(drop=True)\n",
    "\n",
    "df_resampled[\"product_5\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels (LabelEncoder: str â†’ int)\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(df_resampled['product_5'])\n",
    "\n",
    "# Create label mapping (int â†’ str) for interpretation later\n",
    "label_mapping = dict(zip(range(len(label_encoder.classes_)), label_encoder.classes_))\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}  # Optional reverse\n",
    "\n",
    "# Print the mapping for verification\n",
    "print(\"\\nLabel mapping (int â†’ class name):\")\n",
    "for i, label in label_mapping.items():\n",
    "    print(f\"{i}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1801e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    df_resampled['narrative'].values,\n",
    "    encoded_labels,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=encoded_labels\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts,\n",
    "    temp_labels,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=temp_labels\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {len(train_texts)}\")\n",
    "print(f\"Validation set size: {len(val_texts)}\")\n",
    "print(f\"Test set size: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82465a4b",
   "metadata": {},
   "source": [
    "Transformers settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59909d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SELECT MODEL TYPE ===\n",
    "robertA_base = False\n",
    "robertA_base_large = False\n",
    "distilbert = True  # Set the one you want to True\n",
    "\n",
    "# === AUTOMATIC CONFIG BASED ON FLAGS ===\n",
    "if distilbert:\n",
    "    model_name = 'distilbert-base-uncased'\n",
    "    model_type = 'distilbert'\n",
    "    batch_size = 16  # You can keep it higher for smaller models\n",
    "elif robertA_base:\n",
    "    model_name = 'roberta-base'\n",
    "    model_type = 'roberta'\n",
    "    batch_size = 16\n",
    "elif robertA_base_large:\n",
    "    model_name = 'roberta-large'\n",
    "    model_type = 'roberta'\n",
    "    batch_size = 8  # roberta-large is heavy â€” lower the batch size\n",
    "else:\n",
    "    raise ValueError(\"Please set one of the model flags to True.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60043dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class ConsumerComplaintDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        text = text.strip()\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca8779",
   "metadata": {},
   "outputs": [],
   "source": [
    "if distilbert:\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "elif robertA_base:\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "elif robertA_base_large:\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "# Create datasets\n",
    "train_dataset = ConsumerComplaintDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = ConsumerComplaintDataset(val_texts, val_labels, tokenizer)\n",
    "test_dataset = ConsumerComplaintDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "batch_size = 16\n",
    "# Create data loaders\n",
    "if robertA_base_large\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd29ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, epochs=4, learning_rate=5e-5):\n",
    "    # Prepare optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "    \n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    \n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Lists to store loss and accuracy\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        print('-' * 40)\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_dataloader, desc=\"Training\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Clear previous gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip the norm of the gradients to 1.0\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # Update parameters and learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        \n",
    "        for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass without gradient calculation\n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            \n",
    "            # Store predictions and true labels\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        val_accuracy = accuracy_score(true_labels, predictions)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(true_labels, predictions))\n",
    "    \n",
    "    return model, train_losses, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b970602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == 'distilbert':\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(label_mapping),\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False\n",
    "    )\n",
    "elif model_type == 'roberta':\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(label_mapping),\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "# === Training parameters ===\n",
    "epochs = 3\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# === Train the model ===\n",
    "model, train_losses, val_losses, val_accuracies = train_model(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    epochs=epochs,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "# === Save the model ===\n",
    "# Convert model name (e.g., \"distilbert-base-uncased\") into a clean folder name\n",
    "model_save_path = f\"{model_name.replace('/', '_')}_consumer_complaints_model\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass without gradient calculation\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "        \n",
    "        # Get predictions\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Store predictions and true labels\n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predictions))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=label_encoder.classes_, \n",
    "                yticklabels=label_encoder.classes_)\n",
    "    plt.title('Confusion Matrix', fontsize=16)\n",
    "    plt.xlabel('Predicted Label', fontsize=14)\n",
    "    plt.ylabel('True Label', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'true_labels': true_labels,\n",
    "        'predictions': predictions\n",
    "    }\n",
    "\n",
    "# Add missing import for confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63029298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_complaint_category(text, model, tokenizer, label_mapping):\n",
    "    # Tokenize the text\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Forward pass without gradient calculation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Get the predictions\n",
    "    logits = outputs.logits\n",
    "    prediction_id = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    # Get the predicted category\n",
    "    predicted_category = label_mapping[prediction_id]\n",
    "    \n",
    "    return predicted_category\n",
    "\n",
    "# Example usage\n",
    "example_text = \"I am having issues with my credit card. The bank charged me an annual fee even though they said it would be waived.\"\n",
    "predicted_category = predict_complaint_category(example_text, model, tokenizer, label_mapping)\n",
    "print(f\"Predicted category: {predicted_category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example complaints\n",
    "test_complaints = [\n",
    "    \"I am having issues with my credit card. The bank charged me an annual fee even though they said it would be waived.\",\n",
    "    \"My credit report shows incorrect information. There are accounts listed that don't belong to me.\",\n",
    "    \"I requested a loan modification three months ago, but I haven't heard anything back from the lender.\",\n",
    "    \"A debt collector keeps calling me for a debt that isn't mine. I've told them multiple times it's not my debt.\",\n",
    "    \"I cannot access my bank account online. The website keeps showing an error message.\"\n",
    "]\n",
    "\n",
    "# Predict categories\n",
    "for i, complaint in enumerate(test_complaints):\n",
    "    category = predict_complaint_category(complaint, model, tokenizer, label_mapping)\n",
    "    print(f\"Complaint {i+1}: {complaint[:50]}...\")\n",
    "    print(f\"Predicted category: {category}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
